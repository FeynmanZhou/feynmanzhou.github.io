[{"content":"HashiCorp Vault 1.8 brings notable features and improvements to the secrecy and privacy product including Vault Diagnose, integrated-storage autopilot, Key Management secrets engine for AWS, expiration manager improvements, and control-group triggers.\nVault helps users to manage secrets and protect sensitive data using UI, CLI, or HTTP API.\nIn the community office hours of Vault, Stephen Wayne, software engineer of HashiCorp, highlighted the major improvements of the expiration manager and why it does matter for Vault. Expiration manager is used to manage the lifecycle of leases. All dynamic secrets in Vault are required to have a lease.\nVault 1.7 and earlier versions have some obvious limitations especially in revocation, such as leases must be revoked from the system they are associated with, one worker per revocation, irrevocable lease revocation retried on Vault start, and many concurrent revocations consume resources needed by other Vault components. Revocation is critical since it assists in key rolling as well as locking down systems in the case of an intrusion.\nWith Vault 1.8, it has the ability to mark some leases as irrevocable, offers fair-sharing logic to help with lease revocations, and adds an HTTP API and a CLI for operators to obtain information about irrevocable leases. Fortunately, Vault 1.8 reaches the expected outcomes from the end-user perspective, such as more efficient use of resources, more observability into the state of leases, and no more freezes on startup. Now Vault has improved support for lease revocation.\nVault Diagnose has been introduced in Vault 1.8 to enable faster troubleshooting and user-friendly diagnostics when Vault fails to boot or crashes. It means the diagnose command can be used safely regardless of the state Vault is in. Hridoy Roy, software engineer of HashiCorp, walks through the Vault Diagnose command and explains why and how Vault Diagnose is in the community office hours.\nSince the customers are facing the challenges of vault configurations such as misconfigured TLS and certification issues, HashiCorp designed Vault Diagnose to catch some of the common causes for vault misbehavior before they arise. Vault Diagnose uses OpenTelemetry spans to store diagnostic information. It walks the tree and warns, fails, or passes each check with extensive human-readable messages. Hridoy also showed a live demo to introduce the basic use of the diagnose operator command with misconfigured storage or even when Vault is down.\nThe changelog and release notes list all the changes in Vault 1.8. You can also check out the official announcement to find the enterprise features.\n","description":"HashiCorp Vault 1.8 brings notable features and improvements to the secrecy and privacy product including Vault Diagnose, integrated-storage autopilot, Key Management secrets engine for AWS, expiration manager improvements, and control-group triggers.","id":6,"section":"posts","tags":["news"],"title":"HashiCorp Vault 1.8 Adds Diagnose Command, Key Management Secrets Engine, and Expiration Manager","uri":"http://feynmanzhou.github.io/posts/hashicorp-vault-1.8/"},{"content":"Karmada 0.7, featuring a promising Kubernetes management system in the hybrid cloud era, became available on July 12, 2021. It brought multi-cluster service discovery, precise cluster status management, replica scheduling based on cluster resources, and more convenient APIs to divide replicas by weight list.\nKarmada (Kubernetes Armada) is designed for multi-cloud and multi-cluster Kubernetes orchestration with a Kubernetes-native implementation. It provides centralized multi-cloud management, high availability, failure recovery, and traffic scheduling, which enables users to run their cloud-native applications across multiple Kubernetes clusters and clouds with no changes to applications.\nKubernetes Federation is the only official multi-cluster management solution that the SIG Multicluster initiated and maintained. After two major versions, KubeFed has a resilient design and architecture. Karmada is developed in continuation of Kubernetes Federation v1 and v2 and inherited some basic concepts from these two versions. However, Federation v2 (KubeFed) is still in beta and some of the features are still in alpha. Its lack of maturity and community activity are blocking a lot of users from adopting it in production.\nApart from the new features that have been added in this release, the installation scripts support installing Karmada components on both Kind clusters and standalone clusters; other notable improvements related to the Karmada‚Äôs components can be found in the release notes.\nKarmada is fully compatible with the Kubernetes native API as it evolved from KubeFed, so users can deploy their cloud-native applications from single-cluster to multi-cluster with zero changes.\nThis example below demonstrates how to propagate an nginx application to the multi-cluster environments by leveraging the reusable propagation policy. This propagation policy configures the multi-AZ and HA deployment scheme for all target Kubernetes deployments, then users can deploy the application to the multi-cluster environments by speaking Kubernetes-native APIs.\nAs a promising newcomer of the multi-cloud and multi-cluster solutions, it has a new release almost every month. Karmada‚Äôs active contributors include Huawei, ICBC, Tencent, and other organizations so far.\nGoing forward, they plan to add support for more fruitful multi-cluster HA scheduling policies, aggregated Kubernetes API endpoints, multi-cluster service mesh, observability, and GitOps.\nKarmada can be installed within minutes. Get started with Karmada by following the Quick Start and demo. Support is also available via Slack and community meetings.\n","description":"Karmada 0.7, featuring a promising Kubernetes management system in the hybrid cloud era, became available on July 12, 2021. It brought multi-cluster service discovery, precise cluster status management, replica scheduling based on cluster resources, and more convenient APIs to divide replicas by weight list.","id":7,"section":"posts","tags":["news"],"title":"Karmada 0.7 released! The Next-gen Multi-Cloud and Multi-Cluster Kubernetes Orchestration","uri":"http://feynmanzhou.github.io/posts/karmada-07-release/"},{"content":"Litmus, the first chaos engineering project to join the CNCF sandbox program, applies a programmable, declarative approach to chaos engineering. Last month, Litmus 2.0 was released for general availability, with the goal of simplifying chaos engineering by adding new features like chaos center, chaos workflows, GitOps for chaos, multi-tenancy, observability, and private chaos hubs.\nInfoQ interviewed Umasankar Mukkara, CEO of ChaosNative and co-creator and maintainer of Litmus engineering platform.\nInfoQ: Litmus was the first chaos engineering project to join the CNCF. Can you tell us about the technical evolution of Litmus after it became a CNCF sandbox project?\nUmasankar Mukkara: Four years ago, the original idea of Litmus was conceived out of the need for doing chaos engineering in a cloud-native way. The first thing was to build a \u0026ldquo;chaos operator\u0026rdquo; so that the idea of chaos can be managed in a Kubernetes-native way using CRDs. We build chaos experiments as ChaosExperiment custom resources and the operator can run or execute them and store the result in another custom resource called ChaosResult. With this in place, we built a community around chaos experiments and created a public hub called \u0026ldquo;Litmus Chaos Hub\u0026rdquo; for community users. This is the stage at which we donated Litmus to CNCF as a sandbox project. I would call it Litmus 1.0.\nChaos engineering is the process SRE teams and developers willfully engineer faults and observe where there are any issues in the system. So, we introduced the concept of Chaos Center into the Litmus project and made it the central point to coordinate the chaos efforts or reliability efforts in a team or an organization. Chaos Center is also the place where SREs or developers can build complex experiments as a Litmus workflow. A Litmus workflow can be thought of as a complete chaos scenario that gives a definite outcome, whether the system is resilient for a given scenario or not.\nOne of the strong lessons from the community was the requirement to define or build a steady state hypothesis around a chaos experiment. We developed Litmus probes to granularly define and tune the steady state hypothesis in a declarative way. Once you have the ability to construct the chaos scenarios and build the chaos culture in your org, you will reach a stage where you want complete automation around chaos engineering. This is where we introduced Chaos GitOps. With Chaos GitOps, you will be able to trigger a Litmus Workflow after a change is applied to a Kubernetes Deployment or microservice.\nObservability is the key to successful operations. When you introduce chaos engineering, you want to be able to bring the context of it into your existing observability systems. We built Prometheus chaos metrics in such a way you can easily superimpose the context of a chaos experiment onto your Grafana or other similar dashboards.\nAll in all, to summarize the technical evolution of Litmus after it became a CNCF sandbox project, we built Chaos Center, Workflows, GitOps, Litmus probes, and the beginnings of chaos observability.\nInfoQ: Litmus 2.0 introduces chaos workflows to enable users to run multiple experiments together; how can this simplify the chaos engineering practice for users compared to the previous version of the toolkit?\nMukkara: Chaos experiments are like Lego blocks, Litmus workflows are like Lego toys, they are complete and make sense to an end user. Litmus workflows enable users to reuse a lot of work done by their team members and enhance them to build new chaos scenarios.\nChaos center allows rerunning of chaos workflows, cloning of workflows, starting off from a chaos template, and importing a readily shipped workflow. This makes the idea of real chaos engineering easy for users, as you are not starting with a blank slate.\nInfoQ: Litmus 2.0 expands beyond Kubernetes by injecting chaos on infrastructure (cloud) resources such as VMs, bare-metal servers, and disks (AWS, GCP, Azure, VMWare). Will this bring more complexity to the development and maintenance of the project when facing hybrid infrastructure?\nMukkara: The architecture of the Litmus project allows the chaos experiments to be completely independent and plug-and-play. The Litmus platform is written as a Kubernetes application and chaos experiments are wrapped in Kubernetes custom resources. But the experiments themselves need not be limited to targeting Kubernetes resources. This enables Litmus to be used in a real environment where you see cloud-native and cloud-based services coexisting together.\nA chaos scenario or a Litmus workflow can now be defined with chaos experiments targeting a complete hybrid scenario. This makes Litmus more useful and enables practical chaos engineering. We expect more chaos experiments to be contributed in the near future by the community.\nMore details of the Litmus 2.0 release can be found in the release notes.\n","description":"Litmus 2.0 was released for general availability, with the goal of simplifying chaos engineering by adding new features like chaos center, chaos workflows, GitOps for chaos, multi-tenancy, observability, and private chaos hubs.","id":8,"section":"posts","tags":["news"],"title":"Litmus 2.0 Release Includes Multi-Tenancy, Chaos Workflows, GitOps, and Observability","uri":"http://feynmanzhou.github.io/posts/litmus-20-release/"},{"content":"My Installation OS CentOS Linux release 7.6 on QingCloud Platform\nAdd source for docker-ce yum  Extend yum function  $ yum install -y yum-utils Add software source information  $ yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo Automatically select the fastest yum registry source  yum makecache fast List the current version of docker-ce that can be installed  View the version list:  $ yum list docker-ce --showduplicates | sort -r docker-ce.x86_64 3:19.03.9-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.8-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.7-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.6-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.5-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.4-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.3-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.2-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.12-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.11-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.10-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.1-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.0-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.9-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.8-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.7-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.6-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.5-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.4-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.3-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.2-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.1-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.0-3.el7 docker-ce-stable docker-ce.x86_64 18.06.3.ce-3.el7 docker-ce-stable docker-ce.x86_64 18.06.3.ce-3.el7 @docker-ce-stable docker-ce.x86_64 18.06.2.ce-3.el7 docker-ce-stable docker-ce.x86_64 18.06.1.ce-3.el7 docker-ce-stable docker-ce.x86_64 18.06.0.ce-3.el7 docker-ce-stable docker-ce.x86_64 18.03.1.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 18.03.0.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.12.1.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.12.0.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.09.1.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.09.0.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.06.2.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.06.1.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.06.0.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.03.3.ce-1.el7 docker-ce-stable docker-ce.x86_64 17.03.2.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.03.1.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.03.0.ce-1.el7.centos docker-ce-stable Loading mirror speeds from cached hostfile Loaded plugins: fastestmirror Installed Packages Available Packages * updates: mirrors.tuna.tsinghua.edu.cn * extras: mirrors.tuna.tsinghua.edu.cn * epel: mirrors.tuna.tsinghua.edu.cn * base: mirrors.tuna.tsinghua.edu.cn Install the specified version  # yum -y install docker-ce-[VERSION] $ yum install -y docker-ce-18.06.3.ce-3.el7 Start Docker  Start Docker  $ systemctl start docker Verify the docker information:  $ docker info Containers: 0 Running: 0 Paused: 0 Stopped: 0 Images: 0 Server Version: 18.06.3-ce Storage Driver: overlay2 Backing Filesystem: extfs Supports d_type: true Native Overlay Diff: true Logging Driver: json-file Cgroup Driver: cgroupfs Plugins: Volume: local Network: bridge host macvlan null overlay Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog Swarm: inactive Runtimes: runc Default Runtime: runc Init Binary: docker-init containerd version: 468a545b9edcd5932818eb9de8e72413e616e86e runc version: a592beb5bc4c4092b1b1bac971afed27687340c5 init version: fec3683 Security Options: seccomp Profile: default Kernel Version: 3.10.0-957.21.3.el7.x86_64 Operating System: CentOS Linux 7 (Core) OSType: linux Architecture: x86_64 CPUs: 8 Total Memory: 15.51GiB Name: i-3f8hoeou ID: 7MHA:FO5V:YCH6:ED46:KWDZ:OY3E:TE6Y:HRNC:F2KU:GSGF:NYDA:KHYA Docker Root Dir: /var/lib/docker Debug Mode (client): false Debug Mode (server): false Registry: https://index.docker.io/v1/ Labels: Experimental: false Insecure Registries: 127.0.0.0/8 Live Restore Enabled: false At last, let\u0026rsquo;s verify the docker version, it demonstrates the Docker CE has been installed successfully.  $ docker version Client: Version: 18.06.3-ce API version: 1.38 Go version: go1.10.3 Git commit: d7080c1 Built: Wed Feb 20 02:26:51 2019 OS/Arch: linux/amd64 Experimental: false Server: Engine: Version: 18.06.3-ce API version: 1.38 (minimum version 1.12) Go version: go1.10.3 Git commit: d7080c1 Built: Wed Feb 20 02:28:17 2019 OS/Arch: linux/amd64 Experimental: false ","description":"How to Install Docker on CentOS 7.6 in China, with China registry mirror","id":9,"section":"posts","tags":["shortcode"],"title":"How to Install Docker on CentOS in China","uri":"http://feynmanzhou.github.io/posts/install-docker-to-centos-in-china/"},{"content":"Authentication issue After I enabled the Two-factor Authentication in GitHub, I can\u0026rsquo;t push the local commits to GitHub repository, and encountered the issue as follows:\ngit push origin master Password for 'https://git@github.com': remote: Invalid username or password. fatal: Authentication failed for 'https://git@github.com/eurydyce/MDANSE.git/'  How do I Resolve it Why this issue happened after Two-factor Authentication has been enabled? I got the answer from GitHub Documentation.\nHere, I just take notes for the solution. GitHub requires users to create a personal access token to replace the password when performing Git operations over HTTPS with Git on the CLI.\nA personal access token is required to authenticate to GitHub in the following situations:\n When you\u0026rsquo;re using [two-factor authentication](two-factor authentication) To access protected content in an organization that uses SAML single sign-on (SSO). Tokens used with organizations that use SAML SSO must be authorized.  Create a Token   Log in to GitHub, in the upper-right corner of any page, click your profile photo, then click Settings.\n  In the left sidebar, click Developer settings.\n  In the left sidebar, click Personal access tokens.\n  Click Generate new token.\n  Name it, then select the scopes, or permissions, you\u0026rsquo;d like to grant this token. To use your token to access repositories from the command line, select repo.\n   Click Generate token.\n  Copy the token. Then you can enter it instead of your password when performing Git operations over HTTPS.\n  $ git clone https://github.com/username/repo.git Username: your_username Password: your_token # Replace your initial password) It works!\n Tip: Personal access tokens can only be used for HTTPS Git operations. If your repository uses an SSH remote URL, you will need to [switch the remote from SSH to HTTPS](switch the remote from SSH to HTTPS).\n Reference  Stackoverflow - GitHub: invalid username or password Creating a personal access token for the command line  ","description":"How to Resolve GitHub Invalid Authentication","id":10,"section":"posts","tags":["shortcode"],"title":"GitHub Invalid Authentication","uri":"http://feynmanzhou.github.io/posts/github-invalid-authentication/"},{"content":"My Environment CentOS Linux release 7.6 on QingCloud Platform\nConfigure the Docker daemon Either pass the \u0026ndash;registry-mirror option when starting dockerd manually, or edit /etc/docker/daemon.json and add the registry-mirrors key and value, to make the change persistent.\n{ \u0026quot;registry-mirrors\u0026quot;: [\u0026quot;https://\u0026lt;my-docker-mirror-host\u0026gt;\u0026quot;] } Save the file and reload Docker for the change to take effect.\nReload Docker Flush changes and restart Docker:\nsudo systemctl daemon-reload sudo systemctl restart docker Reference  Control Docker with systemd Registry as a pull through cache  ","description":"Add a Registry Mirror in Docker","id":11,"section":"posts","tags":["shortcode"],"title":"How to Add a Registry Mirror in Docker","uri":"http://feynmanzhou.github.io/posts/add-mirror-and-reload-docker/"},{"content":"Intro The Kubernetes command-line tool, kubectl, allows you to run commands against Kubernetes clusters. You can use kubectl to deploy applications, inspect and manage cluster resources, and view logs.\nMy Environment CentOS Linux release 7.6 on QingCloud Platform\nInstall kubectl   For users who can access google api, refer to the Kubernetes Documentation.\n  For users who can not access google api, please refer to the following steps:\n   Add source repository for Kubernetes.  cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF Install kubectl using yum.  yum install -y kubectl Reference  [Blog - Install and Configure Kubernetes] Install and Set Up kubectl  ","description":"How to install Kind to provision a Kubernetes cluster, and install KubeSphere on existing K8s","id":12,"section":"posts","tags":["shortcode"],"title":"Install and Set Up kubectl Tool","uri":"http://feynmanzhou.github.io/posts/install-kubectl/"},{"content":"What is kind kind, Kubernetes IN Docker - local clusters for testing Kubernetes. kind is an open source tool for running local Kubernetes clusters using Docker container ‚Äúnodes‚Äù. kind was primarily designed for testing Kubernetes itself, but may be used for local development. kind bootstraps each \u0026ldquo;node\u0026rdquo; with kubeadm.\nDifferences between kind and kubekey Generally, kind is similar to kubekey, both are used to provision a Kubernetes cluster to the target machines. Specifically, kind has unique scenarios and advantages when comparing with KK, because kind has good cross-platform support, it can provision a standard Kubernetes cluster in Mac/Win/Linux, it\u0026rsquo;s very easy to run KubeSphere anywhere.\nkind runs Kubernetes in a Docker container, with a lightweight runtime containerd. kind packages the necessary dependencies (ebtables, socat, ipset, conntrack, etc.) into one docker image, which gracefully avoid the dependency problem. In addition, Kind will provision a default StorageClass with local storage built in. Typically, for users who want to install KubeSphere for a testing or development in their notebook, kind can used to provision both Kubernetes and KubeSphere in minutes.\nMy Environment CentOS Linux release 7.6 on QingCloud Platform\nPrerequisite  Docker, see How to Install Docker on CentOS in China Kubectl, see Install and Set Up kubectl Tool  Download Kind curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.8.1/kind-$(uname)-amd64 chmod +x ./kind mv ./kind /some-dir-in-your-PATH/kind # e.g. mv ./kind /usr/local/bin/kind Use Kind to Provision Kubernetes  Important: You have to configure the following three things in a configuration file for kind. To configure kind cluster creation, you will need to create a YAML config file. This file follows Kubernetes conventions for versioning etc.\n Create a a YAML config file:  vi config.yaml   Since the APIServer is only listening for 127.0.0.1, which means that APIServer cannot be accessed outside of the Kubernetes cluster which is provisioned by Kind. After creating a cluster, if you want to use kubectl to interact with the Kubernetes cluster by using the kubeconfig, you have to set the filed apiServerAddress to the NodeIP (e.g. 192.168.0.5). See the configuration in networking.\n  For users who download images slowly from DockerHub, it is recommended to configure the image accelerator. See the configuration in containerdConfigPatches.\n  Since Kind will provision things in a docker container, it will cause the service exposing problem. Generally, you need to map ports to the host machine. You can map extra ports from the nodes to the host machine with extraPortMappings.\n  config.yaml\nFill out the config.yaml with the following configuration.\nkind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane extraPortMappings: - containerPort: 30880 # Enter the port you want to map it from the nodes to the host hostPort: 30880 # Enter the port you want to map it from the nodes to the host listenAddress: \u0026quot;0.0.0.0\u0026quot; # Optional, defaults to \u0026quot;0.0.0.0\u0026quot; protocol: tcp # Defaults to tcp, you can use udp as you need networking: apiServerAddress: \u0026quot;192.168.0.5\u0026quot; # Enter the APIServer IP containerdConfigPatches: - |- [plugins.\u0026quot;io.containerd.grpc.v1.cri\u0026quot;.registry.mirrors.\u0026quot;docker.io\u0026quot;] endpoint = [\u0026quot;https://12345.mirror.aliyuncs.com/\u0026quot;] # replace it with your own image accelerator To specify a configuration file when creating a cluster, use the \u0026ndash;config flag:  kind create cluster --config config.yaml Verify the installation:  $ kubectl get pod --all-namespaces kube-system coredns-66bff467f8-984xp 1/1 Running 0 58m kube-system coredns-66bff467f8-bh5sm 1/1 Running 0 58m kube-system etcd-kind-control-plane 1/1 Running 0 59m kube-system kindnet-2t5rx 1/1 Running 0 58m kube-system kube-apiserver-kind-control-plane 1/1 Running 0 59m kube-system kube-controller-manager-kind-control-plane 1/1 Running 3 59m kube-system kube-proxy-g8mqj 1/1 Running 0 58m kube-system kube-scheduler-kind-control-plane 1/1 Running 3 59m kube-system metrics-server-678d5c87c-x4j5k 1/1 local-path-storage local-path-provisioner-bd4bb6b75-xt9zg 1/1 Running 1 58m As you can find that Kind create a default StorageClass local-path to provision local storage. This is quite convenient for the Pods that need to bind PVC.  $ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE standard (default) rancher.io/local-path Delete WaitForFirstConsumer false 61m Install KubeSphere on Kubernetes Cluster At this point, you have a Kubernetes Cluster provisioned by Kind. Next, this is very easy to install KubeSphere on an existing cluster. Plese refer to To Start Deploying KubeSphere.\n Important: KubeSphere v3.0 supports multi-cluster management, you can set the kind cluster as a host or a member cluster as you want. Make sure you configure the cluster role in ClusterRole before deploy KubeSphere. Note that we set the kind cluster as a host in this blog.\n cluster-configuration.yaml\n¬∑¬∑¬∑ multicluster: clusterRole: none # host | member | none ¬∑¬∑¬∑ Import Kind Cluster as host After you installed KubeSphere on an existing Kubernetes cluster, you can import it to the multi-cluster control plane of KubeSphere. Since we set the kind cluster as a host before, we can easily to import the host cluster itself to KubeSphere.\n Obtain the kubeconfig file.  cat $HOME/.kube/config apiVersion: v1 clusters: - cluster: certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJd01EY3dOakl6TVRVek9Wb1hEVE13TURjd05ESXpNVFV6T1Zvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTnJoCk45ejNXVkp4cUxudS84UjQ2SWNla04xOENYbTdOUVhBWGhEZ1MvOUszR2RqczQ0NldDQUNxOE5IbDhKYnE5RXEKZkVVMkZmdUZkTnBGNVpBQm9OaXZzR2tOdkV0QVFqTE5nUDdHTjNxb0FuQ3ZLN05teVFvQWdtVjBBM05PRkZGagpPY0NaTi9qQlNoRWtyR1VYRjlFRGR1cUQvbjdmQytGV1RQOFZaL2ozMFNGMkcySWR2cmdnUU5uZXdrbjE3M00xCnNzc0pRNVZncnRNdEVaaXdjN2ZlT0lEbEsyclJ3NzRYWVR0V256V3VCc2tCYnFIbVNzTi9WNlVVS2xpWG1JWWsKL1NvOEhBK2ZmcVZhQmo4ZzluUVJGY2VleTIzWVdxODcyVndNeGF6WDdaQ3NCTGxpelNaQWpPT09uYnRQODRsMQo5K0lIUys0bjVTRXJxcHNlams4Q0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFFSGJHV2FwRlBNb1NscWw2d2ZQbW91K2xyQU0KajF3dE96ZGkxbUt3QWY1Nm1DRkFmYnhaUURrNCtTUmhieFRCKzFRRzF4L3BJQ0liaFdtbE81OXVEUExPeUl3Vgpkb3A0TEFnTUR2Q1A4ZS9OanRzVnNzT0JiUFdqWmpwU3d0V3ArQU5XWXgyd3ZEQWl2SllmeWVGVE5QVGFXUzdkCmsyb1VMTnBXVTN3cGNhMGZ0d3o2aVUzemhOMytLN3FOcWxvRDRJUjlUazVjdWNHWWlQYytJSzl2cWs5aEJrbk8KVjRuWmYxcVMxUmVQQWRiY292SDJLQU5WN1BrbExOendnQWYzT1BzblNYcVRXT1NMNExGQks4SWpaSXZzaDJSRwprWSt6aWV5OGdFYjhXOUdtWDVFOVE3d0oxNHZJSlhxR0pyRVRhaG9aNHc1cUFMSlA2Q25Db05WaEh1ST0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo= server: https://192.168.0.5:42786 name: kind-kind contexts: - context: cluster: kind-kind user: kind-kind name: kind-kind current-context: kind-kind kind: Config preferences: {} users: - name: kind-kind user: client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM4akNDQWRxZ0F3SUJBZ0lJVmZKOHAvOFJUL0F3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TURBM01EWXlNekUxTXpsYUZ3MHlNVEEzTURZeU16RTFORGhhTURReApGekFWQmdOVkJBb1REbk41YzNSbGJUcHRZWE4wWlhKek1Sa3dGd1lEVlFRREV4QnJkV0psY201bGRHVnpMV0ZrCmJXbHVNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQXdvOWxiV0hWWXVseGNvdkgKT0twZmdQL1dLVm9xNWNSMVhKc0VwT05pcTBUZ2o1ekVGdjJPYnNZMXI1L1YvdXZCYTFSVnVSRmM4UzNHR1p0UQpybHhFc3hqZ0F6a1dlK0Vjd1pnT3dMeFlJSzlCS0dYc21QV012WkxVK0xRZEo4WG9PMG5PM1pyU2JtRy82YnpTCk13S3B6SzAvVUs0V3pxclFncWQ2MmQrWjc0Y0VjRzg5RSs1elBwRmszWkk2VFVtNUlveVkrcWpIT1FJWWhGbDgKY3hVY1RVU1ZGQk5PTldOaDNZZkhCUWdjNjY3TWhubll0ZFIrQ0ZtYWUxTFpGbjRTZm5mZ2M3TjVYSHN0ZDJzbApWY2xyVW5pUkMzTTJQWjlCOHpGRXFuRit2akFLK3BoQmlDMHliTlJJU3ZNUE45TDh2ZTl6akRJS3lObVpzSFNhClNzdFd1UUlEQVFBQm95Y3dKVEFPQmdOVkhROEJBZjhFQkFNQ0JhQXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFOblZvUWJyQmJXaW1ZbU5HeUJXQlZTY0kwMXNQaVJiS0xLUgpFTTUxYWQ3SFoxR01PaDY3YzhEVno3TVZtMUtjU2RhaituUlMzcnRWdzZicCt0M1NXTTdwUXJEME9zUlZ4bGVvClRQTktDbGJ5NVp6bUFjNVJ1TzBmM0hmdDVjS0x1ZkEwR0lkWWVTOE9HL2ZVSjZoaktqbkxjWkRQN1dBbTczajMKVG5ZaU9RMjIrQXF5QVNYUkhkVWpQMkJZNEIrZUI5TFd6ejQ5Z1lTTExjUk50a2FmQjlaRHZkd1padFlBRFcrNApWQjUwNDBFV3NRajloT0doQVJmVjNVZG81TTVoSHRSNFA0RFJRK1dCQUt0YjZlMmp6eEpNVjB5YkRjVWl6MVZRCklRVjZkeVdEN3lqTFhUWXZlVExiSGhxanl0V04rdTgrYjk3NzRXRzdZYUpYekZJR3lLTT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo= client-key-data: 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 Back to KubeSphere, click Add Cluster.  Fill out the cluster name, and choose the Tag and Provider as you want, then click Next.   Copy and paste the kubeconfig to the multi-cluster control plane of KubeSphere:\n  Keep the Connection Method as Direct connection to Kubernetes cluster. Copy and paste the kubeconfig into the blank, then click Import to import the target cluster into KubeSphere.\n  Wait for seconds, the cluster will be joined in a while.  Congratulation! When you see the cluster has imported into KubeSphere, you can enjoy it!  Import a member cluster Same as above, we can continue to import a member cluster into multi-cluster control plane of KubeSphere.\nCool! Wait for seconds, the member cluster dashboard is coming.\nReference  kind website containerd website Faster than Minikube, using Kind to quickly create a K8s learning environment kubekey  ","description":"How to install Kind to provision a Kubernetes cluster, and install KubeSphere on existing K8s","id":13,"section":"posts","tags":["shortcode"],"title":"Use Kind to provision Kubernetes, run KubeSphere anywhere","uri":"http://feynmanzhou.github.io/posts/install-kind-k8s-and-kubesphere/"},{"content":"Record the conferences and events in my work and life.\n","description":"photo gallery","id":14,"section":"gallery","tags":null,"title":"Photos","uri":"http://feynmanzhou.github.io/gallery/photo/"},{"content":"Hi, this is Feynman üëã 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  apiVersion:feynmanzhou.github.io/v1alpha1kind:introductionmetadata:name:aboutannotations:feynmanzhou.github.io/full-nameüòä:FeynmanZhouÔºàÂë®ÈπèÈ£ûÔºâfeynmanzhou.github.io/organizationüè°:KubeSphere(https://kubesphere.io)feynmanzhou.github.io/careerüíª:CommunityManagerfeynmanzhou.github.io/communityü•ë:CNCFAmbassador,FluentMember,InfoQEditorfeynmanzhou.github.io/companyüè¢:QingCloudfeynmanzhou.github.io/websiteüë®‚Äçüíª:https://feynmanzhou.github.iospec:skills:- Kubernetes,Fluentd\u0026amp;FluentBit,DevOps,Serverless/FaaS- TechnicalWriting,AdvocacyandOutreach,HostEvents  Feynman is a community manager at QingCloud, he is growing and maintaining the KubeSphere open source community, which helps users to widely adopt Kubernetes and reduce the learning curve of using cloud-native technologies. He focuses on advocacy and outreach, community growth, documentation, and user case studies.\nBesides, Feynman is a CNCF Ambassador, Feynman helps CNCF to promote cloud native technologies in China. He has hosted several online or in-person campaigns, such as Kubernetes Community Days China, CNCF Webinar, and cloud native meetups.\nFeynman is also an InfoQ DevOps Editor. You can find the blogs and news writen by Feynman on his InfoQ profile. Feel free to contact Feynman to write a piece of news for your project if you want.\nWhen Feynman is not working, he enjoys playing basketball, hiking, reading, listening podcasts, etc. Feynman is interested in any kind of sport.\nWant to have a talk or have a cup of coffee with Feynman? Book a slot with Feynman on Calendly. If you are working in China, you can connect with Feynman via WeChat ID 493200090.\n","description":"Hugo, the world‚Äôs fastest framework for building websites","id":15,"section":"","tags":null,"title":"About","uri":"http://feynmanzhou.github.io/about/"}]