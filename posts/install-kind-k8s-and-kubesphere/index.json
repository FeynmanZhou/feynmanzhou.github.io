[{"content":"My Installation OS CentOS Linux release 7.6 on QingCloud Platform\nAdd source for docker-ce yum  Extend yum function  $ yum install -y yum-utils Add software source information  $ yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo Automatically select the fastest yum registry source  yum makecache fast List the current version of docker-ce that can be installed  View the version list:  $ yum list docker-ce --showduplicates | sort -r docker-ce.x86_64 3:19.03.9-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.8-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.7-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.6-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.5-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.4-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.3-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.2-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.12-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.11-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.10-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.1-3.el7 docker-ce-stable docker-ce.x86_64 3:19.03.0-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.9-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.8-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.7-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.6-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.5-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.4-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.3-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.2-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.1-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.0-3.el7 docker-ce-stable docker-ce.x86_64 18.06.3.ce-3.el7 docker-ce-stable docker-ce.x86_64 18.06.3.ce-3.el7 @docker-ce-stable docker-ce.x86_64 18.06.2.ce-3.el7 docker-ce-stable docker-ce.x86_64 18.06.1.ce-3.el7 docker-ce-stable docker-ce.x86_64 18.06.0.ce-3.el7 docker-ce-stable docker-ce.x86_64 18.03.1.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 18.03.0.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.12.1.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.12.0.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.09.1.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.09.0.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.06.2.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.06.1.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.06.0.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.03.3.ce-1.el7 docker-ce-stable docker-ce.x86_64 17.03.2.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.03.1.ce-1.el7.centos docker-ce-stable docker-ce.x86_64 17.03.0.ce-1.el7.centos docker-ce-stable Loading mirror speeds from cached hostfile Loaded plugins: fastestmirror Installed Packages Available Packages * updates: mirrors.tuna.tsinghua.edu.cn * extras: mirrors.tuna.tsinghua.edu.cn * epel: mirrors.tuna.tsinghua.edu.cn * base: mirrors.tuna.tsinghua.edu.cn Install the specified version  # yum -y install docker-ce-[VERSION] $ yum install -y docker-ce-18.06.3.ce-3.el7 Start Docker  Start Docker  $ systemctl start docker Verify the docker information:  $ docker info Containers: 0 Running: 0 Paused: 0 Stopped: 0 Images: 0 Server Version: 18.06.3-ce Storage Driver: overlay2 Backing Filesystem: extfs Supports d_type: true Native Overlay Diff: true Logging Driver: json-file Cgroup Driver: cgroupfs Plugins: Volume: local Network: bridge host macvlan null overlay Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslog Swarm: inactive Runtimes: runc Default Runtime: runc Init Binary: docker-init containerd version: 468a545b9edcd5932818eb9de8e72413e616e86e runc version: a592beb5bc4c4092b1b1bac971afed27687340c5 init version: fec3683 Security Options: seccomp Profile: default Kernel Version: 3.10.0-957.21.3.el7.x86_64 Operating System: CentOS Linux 7 (Core) OSType: linux Architecture: x86_64 CPUs: 8 Total Memory: 15.51GiB Name: i-3f8hoeou ID: 7MHA:FO5V:YCH6:ED46:KWDZ:OY3E:TE6Y:HRNC:F2KU:GSGF:NYDA:KHYA Docker Root Dir: /var/lib/docker Debug Mode (client): false Debug Mode (server): false Registry: https://index.docker.io/v1/ Labels: Experimental: false Insecure Registries: 127.0.0.0/8 Live Restore Enabled: false At last, let\u0026rsquo;s verify the docker version, it demonstrates the Docker CE has been installed successfully.  $ docker version Client: Version: 18.06.3-ce API version: 1.38 Go version: go1.10.3 Git commit: d7080c1 Built: Wed Feb 20 02:26:51 2019 OS/Arch: linux/amd64 Experimental: false Server: Engine: Version: 18.06.3-ce API version: 1.38 (minimum version 1.12) Go version: go1.10.3 Git commit: d7080c1 Built: Wed Feb 20 02:28:17 2019 OS/Arch: linux/amd64 Experimental: false ","description":"How to Install Docker on CentOS 7.6 in China, with China registry mirror","id":2,"section":"posts","tags":["shortcode"],"title":"How to Install Docker on CentOS in China","uri":"http://feynmanzhou.github.io/posts/install-docker-to-centos-in-china/"},{"content":"Authentication issue After I enabled the Two-factor Authentication in GitHub, I can\u0026rsquo;t push the local commits to GitHub repository, and encountered the issue as follows:\ngit push origin master Password for 'https://git@github.com': remote: Invalid username or password. fatal: Authentication failed for 'https://git@github.com/eurydyce/MDANSE.git/'  How do I Resolve it Why this issue happened after Two-factor Authentication has been enabled? I got the answer from GitHub Documentation.\nHere, I just take notes for the solution. GitHub requires users to create a personal access token to replace the password when performing Git operations over HTTPS with Git on the CLI.\nA personal access token is required to authenticate to GitHub in the following situations:\n When you\u0026rsquo;re using [two-factor authentication](two-factor authentication) To access protected content in an organization that uses SAML single sign-on (SSO). Tokens used with organizations that use SAML SSO must be authorized.  Create a Token   Log in to GitHub, in the upper-right corner of any page, click your profile photo, then click Settings.\n  In the left sidebar, click Developer settings.\n  In the left sidebar, click Personal access tokens.\n  Click Generate new token.\n  Name it, then select the scopes, or permissions, you\u0026rsquo;d like to grant this token. To use your token to access repositories from the command line, select repo.\n   Click Generate token.\n  Copy the token. Then you can enter it instead of your password when performing Git operations over HTTPS.\n  $ git clone https://github.com/username/repo.git Username: your_username Password: your_token # Replace your initial password) It works!\n Tip: Personal access tokens can only be used for HTTPS Git operations. If your repository uses an SSH remote URL, you will need to [switch the remote from SSH to HTTPS](switch the remote from SSH to HTTPS).\n Reference  Stackoverflow - GitHub: invalid username or password Creating a personal access token for the command line  ","description":"How to Resolve GitHub Invalid Authentication","id":3,"section":"posts","tags":["shortcode"],"title":"GitHub Invalid Authentication","uri":"http://feynmanzhou.github.io/posts/github-invalid-authentication/"},{"content":"My Environment CentOS Linux release 7.6 on QingCloud Platform\nConfigure the Docker daemon Either pass the \u0026ndash;registry-mirror option when starting dockerd manually, or edit /etc/docker/daemon.json and add the registry-mirrors key and value, to make the change persistent.\n{ \u0026quot;registry-mirrors\u0026quot;: [\u0026quot;https://\u0026lt;my-docker-mirror-host\u0026gt;\u0026quot;] } Save the file and reload Docker for the change to take effect.\nReload Docker Flush changes and restart Docker:\nsudo systemctl daemon-reload sudo systemctl restart docker Reference  Control Docker with systemd Registry as a pull through cache  ","description":"Add a Registry Mirror in Docker","id":4,"section":"posts","tags":["shortcode"],"title":"How to Add a Registry Mirror in Docker","uri":"http://feynmanzhou.github.io/posts/add-mirror-and-reload-docker/"},{"content":"Intro The Kubernetes command-line tool, kubectl, allows you to run commands against Kubernetes clusters. You can use kubectl to deploy applications, inspect and manage cluster resources, and view logs.\nMy Environment CentOS Linux release 7.6 on QingCloud Platform\nInstall kubectl   For users who can access google api, refer to the Kubernetes Documentation.\n  For users who can not access google api, please refer to the following steps:\n   Add source repository for Kubernetes.  cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF Install kubectl using yum.  yum install -y kubectl Reference  [Blog - Install and Configure Kubernetes] Install and Set Up kubectl  ","description":"How to install Kind to provision a Kubernetes cluster, and install KubeSphere on existing K8s","id":5,"section":"posts","tags":["shortcode"],"title":"Install and Set Up kubectl Tool","uri":"http://feynmanzhou.github.io/posts/install-kubectl/"},{"content":"What is kind kind, Kubernetes IN Docker - local clusters for testing Kubernetes. kind is an open source tool for running local Kubernetes clusters using Docker container “nodes”. kind was primarily designed for testing Kubernetes itself, but may be used for local development. kind bootstraps each \u0026ldquo;node\u0026rdquo; with kubeadm.\nDifferences between kind and kubekey Generally, kind is similar to kubekey, both are used to provision a Kubernetes cluster to the target machines. Specifically, kind has unique scenarios and advantages when comparing with KK, because kind has good cross-platform support, it can provision a standard Kubernetes cluster in Mac/Win/Linux, it\u0026rsquo;s very easy to run KubeSphere anywhere.\nkind runs Kubernetes in a Docker container, with a lightweight runtime containerd. kind packages the necessary dependencies (ebtables, socat, ipset, conntrack, etc.) into one docker image, which gracefully avoid the dependency problem. In addition, Kind will provision a default StorageClass with local storage built in. Typically, for users who want to install KubeSphere for a testing or development in their notebook, kind can used to provision both Kubernetes and KubeSphere in minutes.\nMy Environment CentOS Linux release 7.6 on QingCloud Platform\nPrerequisite  Docker, see How to Install Docker on CentOS in China Kubectl, see Install and Set Up kubectl Tool  Download Kind curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.8.1/kind-$(uname)-amd64 chmod +x ./kind mv ./kind /some-dir-in-your-PATH/kind # e.g. mv ./kind /usr/local/bin/kind Use Kind to Provision Kubernetes  Important: You have to configure the following three things in a configuration file for kind. To configure kind cluster creation, you will need to create a YAML config file. This file follows Kubernetes conventions for versioning etc.\n Create a a YAML config file:  vi config.yaml   Since the APIServer is only listening for 127.0.0.1, which means that APIServer cannot be accessed outside of the Kubernetes cluster which is provisioned by Kind. After creating a cluster, if you want to use kubectl to interact with the Kubernetes cluster by using the kubeconfig, you have to set the filed apiServerAddress to the NodeIP (e.g. 192.168.0.5). See the configuration in networking.\n  For users who download images slowly from DockerHub, it is recommended to configure the image accelerator. See the configuration in containerdConfigPatches.\n  Since Kind will provision things in a docker container, it will cause the service exposing problem. Generally, you need to map ports to the host machine. You can map extra ports from the nodes to the host machine with extraPortMappings.\n  config.yaml\nFill out the config.yaml with the following configuration.\nkind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane extraPortMappings: - containerPort: 30880 # Enter the port you want to map it from the nodes to the host hostPort: 30880 # Enter the port you want to map it from the nodes to the host listenAddress: \u0026quot;0.0.0.0\u0026quot; # Optional, defaults to \u0026quot;0.0.0.0\u0026quot; protocol: tcp # Defaults to tcp, you can use udp as you need networking: apiServerAddress: \u0026quot;192.168.0.5\u0026quot; # Enter the APIServer IP containerdConfigPatches: - |- [plugins.\u0026quot;io.containerd.grpc.v1.cri\u0026quot;.registry.mirrors.\u0026quot;docker.io\u0026quot;] endpoint = [\u0026quot;https://12345.mirror.aliyuncs.com/\u0026quot;] # replace it with your own image accelerator To specify a configuration file when creating a cluster, use the \u0026ndash;config flag:  kind create cluster --config config.yaml Verify the installation:  $ kubectl get pod --all-namespaces kube-system coredns-66bff467f8-984xp 1/1 Running 0 58m kube-system coredns-66bff467f8-bh5sm 1/1 Running 0 58m kube-system etcd-kind-control-plane 1/1 Running 0 59m kube-system kindnet-2t5rx 1/1 Running 0 58m kube-system kube-apiserver-kind-control-plane 1/1 Running 0 59m kube-system kube-controller-manager-kind-control-plane 1/1 Running 3 59m kube-system kube-proxy-g8mqj 1/1 Running 0 58m kube-system kube-scheduler-kind-control-plane 1/1 Running 3 59m kube-system metrics-server-678d5c87c-x4j5k 1/1 local-path-storage local-path-provisioner-bd4bb6b75-xt9zg 1/1 Running 1 58m As you can find that Kind create a default StorageClass local-path to provision local storage. This is quite convenient for the Pods that need to bind PVC.  $ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE standard (default) rancher.io/local-path Delete WaitForFirstConsumer false 61m Install KubeSphere on Kubernetes Cluster At this point, you have a Kubernetes Cluster provisioned by Kind. Next, this is very easy to install KubeSphere on an existing cluster. Plese refer to To Start Deploying KubeSphere.\n Important: KubeSphere v3.0 supports multi-cluster management, you can set the kind cluster as a host or a member cluster as you want. Make sure you configure the cluster role in ClusterRole before deploy KubeSphere. Note that we set the kind cluster as a host in this blog.\n cluster-configuration.yaml\n··· multicluster: clusterRole: none # host | member | none ··· Import Kind Cluster as host After you installed KubeSphere on an existing Kubernetes cluster, you can import it to the multi-cluster control plane of KubeSphere. Since we set the kind cluster as a host before, we can easily to import the host cluster itself to KubeSphere.\n Obtain the kubeconfig file.  cat $HOME/.kube/config apiVersion: v1 clusters: - cluster: certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJd01EY3dOakl6TVRVek9Wb1hEVE13TURjd05ESXpNVFV6T1Zvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTnJoCk45ejNXVkp4cUxudS84UjQ2SWNla04xOENYbTdOUVhBWGhEZ1MvOUszR2RqczQ0NldDQUNxOE5IbDhKYnE5RXEKZkVVMkZmdUZkTnBGNVpBQm9OaXZzR2tOdkV0QVFqTE5nUDdHTjNxb0FuQ3ZLN05teVFvQWdtVjBBM05PRkZGagpPY0NaTi9qQlNoRWtyR1VYRjlFRGR1cUQvbjdmQytGV1RQOFZaL2ozMFNGMkcySWR2cmdnUU5uZXdrbjE3M00xCnNzc0pRNVZncnRNdEVaaXdjN2ZlT0lEbEsyclJ3NzRYWVR0V256V3VCc2tCYnFIbVNzTi9WNlVVS2xpWG1JWWsKL1NvOEhBK2ZmcVZhQmo4ZzluUVJGY2VleTIzWVdxODcyVndNeGF6WDdaQ3NCTGxpelNaQWpPT09uYnRQODRsMQo5K0lIUys0bjVTRXJxcHNlams4Q0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFFSGJHV2FwRlBNb1NscWw2d2ZQbW91K2xyQU0KajF3dE96ZGkxbUt3QWY1Nm1DRkFmYnhaUURrNCtTUmhieFRCKzFRRzF4L3BJQ0liaFdtbE81OXVEUExPeUl3Vgpkb3A0TEFnTUR2Q1A4ZS9OanRzVnNzT0JiUFdqWmpwU3d0V3ArQU5XWXgyd3ZEQWl2SllmeWVGVE5QVGFXUzdkCmsyb1VMTnBXVTN3cGNhMGZ0d3o2aVUzemhOMytLN3FOcWxvRDRJUjlUazVjdWNHWWlQYytJSzl2cWs5aEJrbk8KVjRuWmYxcVMxUmVQQWRiY292SDJLQU5WN1BrbExOendnQWYzT1BzblNYcVRXT1NMNExGQks4SWpaSXZzaDJSRwprWSt6aWV5OGdFYjhXOUdtWDVFOVE3d0oxNHZJSlhxR0pyRVRhaG9aNHc1cUFMSlA2Q25Db05WaEh1ST0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo= server: https://192.168.0.5:42786 name: kind-kind contexts: - context: cluster: kind-kind user: kind-kind name: kind-kind current-context: kind-kind kind: Config preferences: {} users: - name: kind-kind user: client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM4akNDQWRxZ0F3SUJBZ0lJVmZKOHAvOFJUL0F3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TURBM01EWXlNekUxTXpsYUZ3MHlNVEEzTURZeU16RTFORGhhTURReApGekFWQmdOVkJBb1REbk41YzNSbGJUcHRZWE4wWlhKek1Sa3dGd1lEVlFRREV4QnJkV0psY201bGRHVnpMV0ZrCmJXbHVNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQXdvOWxiV0hWWXVseGNvdkgKT0twZmdQL1dLVm9xNWNSMVhKc0VwT05pcTBUZ2o1ekVGdjJPYnNZMXI1L1YvdXZCYTFSVnVSRmM4UzNHR1p0UQpybHhFc3hqZ0F6a1dlK0Vjd1pnT3dMeFlJSzlCS0dYc21QV012WkxVK0xRZEo4WG9PMG5PM1pyU2JtRy82YnpTCk13S3B6SzAvVUs0V3pxclFncWQ2MmQrWjc0Y0VjRzg5RSs1elBwRmszWkk2VFVtNUlveVkrcWpIT1FJWWhGbDgKY3hVY1RVU1ZGQk5PTldOaDNZZkhCUWdjNjY3TWhubll0ZFIrQ0ZtYWUxTFpGbjRTZm5mZ2M3TjVYSHN0ZDJzbApWY2xyVW5pUkMzTTJQWjlCOHpGRXFuRit2akFLK3BoQmlDMHliTlJJU3ZNUE45TDh2ZTl6akRJS3lObVpzSFNhClNzdFd1UUlEQVFBQm95Y3dKVEFPQmdOVkhROEJBZjhFQkFNQ0JhQXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFOblZvUWJyQmJXaW1ZbU5HeUJXQlZTY0kwMXNQaVJiS0xLUgpFTTUxYWQ3SFoxR01PaDY3YzhEVno3TVZtMUtjU2RhaituUlMzcnRWdzZicCt0M1NXTTdwUXJEME9zUlZ4bGVvClRQTktDbGJ5NVp6bUFjNVJ1TzBmM0hmdDVjS0x1ZkEwR0lkWWVTOE9HL2ZVSjZoaktqbkxjWkRQN1dBbTczajMKVG5ZaU9RMjIrQXF5QVNYUkhkVWpQMkJZNEIrZUI5TFd6ejQ5Z1lTTExjUk50a2FmQjlaRHZkd1padFlBRFcrNApWQjUwNDBFV3NRajloT0doQVJmVjNVZG81TTVoSHRSNFA0RFJRK1dCQUt0YjZlMmp6eEpNVjB5YkRjVWl6MVZRCklRVjZkeVdEN3lqTFhUWXZlVExiSGhxanl0V04rdTgrYjk3NzRXRzdZYUpYekZJR3lLTT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo= client-key-data: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFb3dJQkFBS0NBUUVBd285bGJXSFZZdWx4Y292SE9LcGZnUC9XS1ZvcTVjUjFYSnNFcE9OaXEwVGdqNXpFCkZ2Mk9ic1kxcjUvVi91dkJhMVJWdVJGYzhTM0dHWnRRcmx4RXN4amdBemtXZStFY3daZ093THhZSUs5QktHWHMKbVBXTXZaTFUrTFFkSjhYb08wbk8zWnJTYm1HLzZielNNd0twekswL1VLNFd6cXJRZ3FkNjJkK1o3NGNFY0c4OQpFKzV6UHBGazNaSTZUVW01SW95WStxakhPUUlZaEZsOGN4VWNUVVNWRkJOT05XTmgzWWZIQlFnYzY2N01obm5ZCnRkUitDRm1hZTFMWkZuNFNmbmZnYzdONVhIc3RkMnNsVmNsclVuaVJDM00yUFo5Qjh6RkVxbkYrdmpBSytwaEIKaUMweWJOUklTdk1QTjlMOHZlOXpqRElLeU5tWnNIU2FTc3RXdVFJREFRQUJBb0lCQVFDOEV0SUJRcWw3ekI0VwpsSGdvbHNscmNlUWNaVVZDaHY3TVhpM0hGdWV5bUFwYnlUYms0b1psSHNXVzEvT05VV2pQejk1dDRCTHdNVWRtClNYVmNsMlR4bkFJd0cyZFlxT3ViV05vcUJlZEs4UjhUWmpPb1NQV2I2K3hqM2ZpeEVlYkVLVDBIaXpvUDZmNFMKZkt6VkVxWUJiUmZmYmVvdUxZRVNTOE8rQ1NCTklZNGtGZk9CbGNUeGZQWEFVZGJDelpwWUJyaC81dVc4dldEZQpMNVo3OThZc3BiZGplczZkRVlXeXE4UUZJVE5UbnBOZ1g3aXVCc1BsR1o2SWxGT3lsaTdmcFliSFpWenhBcnFMCkNTRG9xMjJZTEpQUHE5MGtmZ2oxbC9WUDRVcE83dGV6VXdnZy9lN3Z2YmR1M3F2K1c4eW1GQzA4ekc0ZnllbnQKRnhNTWdPQUJBb0dCQVBTcEYyUVZSN25nbVJRbzRIem8wM1p2clVpblpDaWJZdzEzUEliN0tkUmRUeUd6ZXhOcQowRzRkTVhvU3VIYSsvUWRZUFd6RmxqK29YcmFab2M4KzJEdlh1Y0RnK0RSUThhNWNBQ21PRnE2MEREWlc5aGpECkp5YSs0MzFIaXJId1NDR1dXRUV5S0VibUxES0FIdjlER09vejFSTllQNXZneHJUS0I2NWZiSHVCQW9HQkFNdVQKM0kyTGJuc0N2QTlxYUdTU0R1T3Fmc2hENGZkVzZLbUJYRHlqaVNCcVhkb0MvNG9POGh6U2lBNit3cUt5TXRNdQp2YUV3elpOQU5sdVVyU1JTcFVoenh1dzNybHFFV0hsVVhTR3BFUk10Uk5XUVlXUWsrNS9Ea3lRN0pRVnczUDJXCllPM0wwK2wvci9ud29HeU5xdzkxOWJRR0VWdlEvZkJwbzFQdksxYzVBb0dBS0pSYk10aHN1eUhMZ2hKOW1Yb3MKaUxwaExXbkdMUTRJMGRUekR4aGRpY0dvUEZpK2t5dm1RajFKVll3QldJQzVDeHpSbzFicVlzaVVYUWtDVTVPQwozZm9iN3lmaFF4d2gyZCtjajBmbjd6WWh5R21JQy9kaWFRVEVTcVV4RmU1YXFHZDlYK0xuMTBxYURnNCtGZXlWCkYxTmRoZ1hmRndXakw1MUt4TnRDN1FFQ2dZQkNCS2hnaXBnQWNrbVVZamlLYk1kQlUzZ2I3OCtteTF0V0pPcEoKaStzYlJRTThnUDVud2lNSDc4cEpwZGt1czhWQnNRV3o4VVNwZlFoanVKcFJqaStsaUU0NUtuRnpUem4xMXNNQQorSGZlRlM4ZVp0eitnZlBMd3J2RDR4NUZYbTE4R3psQnhIanJYd290YnJoSG04V2VsTzFpUFJtQ0FndG4zSW9uCjNRYmNHUUtCZ0RYQytvZ3Facjlxa0h1SUZZQWYwL0hPRmhBLzFxOTQ5QXpEY3hiRnQzNUlzVUtxWDY0Z3Q1U2IKZFpNN0ZtdW1DK3BFV3pQaTF3aGc2eUtWeHlobmZHdStlYUpIaVN4VStEaDVnVkJNSjEwNzdUZVhMVWZRUk9ldgpzNVZ0ank1dFp2bW4xK1NHNnZLcG5RUWJNWUNsWmQzYmE4TXpNSGZvUlN3VU1MMDdVemtiCi0tLS0tRU5EIFJTQSBQUklWQVRFIEtFWS0tLS0tCg== Back to KubeSphere, click Add Cluster.  Fill out the cluster name, and choose the Tag and Provider as you want, then click Next.   Copy and paste the kubeconfig to the multi-cluster control plane of KubeSphere:\n  Keep the Connection Method as Direct connection to Kubernetes cluster. Copy and paste the kubeconfig into the blank, then click Import to import the target cluster into KubeSphere.\n  Wait for seconds, the cluster will be joined in a while.  Congratulation! When you see the cluster has imported into KubeSphere, you can enjoy it!  Import a member cluster Same as above, we can continue to import a member cluster into multi-cluster control plane of KubeSphere.\nCool! Wait for seconds, the member cluster dashboard is coming.\nReference  kind website containerd website Faster than Minikube, using Kind to quickly create a K8s learning environment kubekey  ","description":"How to install Kind to provision a Kubernetes cluster, and install KubeSphere on existing K8s","id":6,"section":"posts","tags":["shortcode"],"title":"Use Kind to provision Kubernetes, run KubeSphere anywhere","uri":"http://feynmanzhou.github.io/posts/install-kind-k8s-and-kubesphere/"},{"content":"Sample images\n","description":"cartoon gallery","id":7,"section":"gallery","tags":null,"title":"Cartoon","uri":"http://feynmanzhou.github.io/gallery/cartoon/"},{"content":"Sample images from Pixabay\n","description":"photo gallery","id":8,"section":"gallery","tags":null,"title":"Photo","uri":"http://feynmanzhou.github.io/gallery/photo/"},{"content":"Written in Go, Hugo is an open source static site generator available under the Apache Licence 2.0. Hugo supports TOML, YAML and JSON data file types, Markdown and HTML content files and uses shortcodes to add rich content. Other notable features are taxonomies, multilingual mode, image processing, custom output formats, HTML/CSS/JS minification and support for Sass SCSS workflows.\nHugo makes use of a variety of open source projects including:\n https://github.com/russross/blackfriday https://github.com/alecthomas/chroma https://github.com/muesli/smartcrop https://github.com/spf13/cobra https://github.com/spf13/viper  Hugo is ideal for blogs, corporate websites, creative portfolios, online magazines, single page applications or even a website with thousands of pages.\nHugo is for people who want to hand code their own website without worrying about setting up complicated runtimes, dependencies and databases.\nWebsites built with Hugo are extremelly fast, secure and can be deployed anywhere including, AWS, GitHub Pages, Heroku, Netlify and any other hosting provider.\nLearn more and contribute on GitHub.\n","description":"Hugo, the world’s fastest framework for building websites","id":13,"section":"","tags":null,"title":"About","uri":"http://feynmanzhou.github.io/about/"}]